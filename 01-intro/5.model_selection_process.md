## How to select the best model?

- Logistic regression
- Decision tree
- Neural network

**Selecting the best model**

We train the model and test it on new data. How do we test it before deploying it?

We split our data into train and test (validation) sets.

![alt text](https://github.com/sebastian2296/ml-zoomcamp/blob/main/01-intro/img/split.png)

We train our model with the training dataset with its' corresponding feature and label matrixes. The output will be our model *G*.

For the validation dataset we have our corresponding matrixes, then apply the *G* model to the features matrix (validation) and from here we get a predicted vector for our labels.

We now need to compare the predictions with our actual values (predicted vector with actual labels).
The predicted vector is a **probabilities vector**. We need to convert it to the binary outcome using a threshold. 

Once this is done, we can compute the accuracy of our model, which is just the ratio between successfully classified emails and total amount of emails (66% percent in the video).

Stepping back, which model should we use?

- Logistic Regression: 66% accurate
- Decision Tree: 60% accurate
- Random Forest: 67% accurate
- Neural Network: 80% accurate

According to this, we should pick Neural Network as our model to deploy. But there could be a }"multiple comparisons" problem: let's say our model could be represented by 5 coins (each one is a model). Heads classifies an email as spam and tails as not spam. In 5 different iterations we could get totally different results (even if we repeat the experiment). 

In the example above, it could be that the Neural Network "coin" got a better score by pure chance.

To guard against this happening, we divide our dataset into **train** (60%), **Validation** (20%), **Test* (20%). We put way the test dataset. Then we repeat the process described above; build the model with the training dataset and then we apply *G* to the validation dataset. Ultimately, we apply the model to the test dataset to make sure that it did not get lucky and it's actually the best model.

### Model Selection Process

1. Split dataset
2. Train the model
3. Apply the model to the validation dataset (with as many models as we like)
4. Select the best model from step 3 and we apply it to the test dataset
5. Check if the score in the test dataset is similar to the validation dataset

### A note on the validation dataset

We could argue that we are wasting the validation dataset in the sense that is not used for training the model (missing patterns). What we can do is join the validation and train datasets, re train the model and then evaluate it the test dataset.

![alt text](https://github.com/sebastian2296/ml-zoomcamp/blob/main/01-intro/img/re_train.png)