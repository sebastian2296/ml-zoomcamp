## What is CRISP-DM

It's a methodology for organizing ML projects. It stands for *Cross Industry Standard Processing Data Mining*. It goes from problem understanding to deployment.


### Understand the problem

- Collect the data

- Train the model

- Use it

**Spam Example**

We get an email, extract some features, put it in the model and based on the score it gives (and the threshold) we classifiy either as spam or not.

How to use Crisp-DM in this example?

![alt text](https://github.com/sebastian2296/ml-zoomcamp/blob/main/01-intro/img/crisp-dm.png)

### 1. Business understanding

We want to understand if this is important and identify how we would measure the success of the problem. More importantly, do we need machine learning to solve it?

In the context of the spam detector:

- Our users complain about spam (so what).
- Analyze to what extent it's a problem.
- Do we really need machine learning to solve it? (we could use a rule based system instead)
- If not: propose an alternative solution.

Define the goal:

- Reduce the amount of spam messages or
- Reduce the amount of complaints about spam

The goal has to be measurable:

- Reduce the amount of spam by 50%.

### Data Understanding

Analyze available data sources, decide if we need to get more data. 

Identify data sources:

- Spam Button
- Is the data behind this button good enough? 
- Is it reliable (users might click the spam button when in reality they are not).
- Is the dataset large enough?
- Do we need to get more data?

If we use wrong data, the model will mimic something that does not represent reality. 

In this step we can learn more information about the problem. 

### Data Preparation

We already analyzed the data (it's reliable). 

We take this data so that it can be used as input by a ML model. 

- Clean the data.
- Build pipelines (sequence of steps to transform the data).
- Convert it into tabular form.

### Modeling

Once we have our data in tabular format, we can train our model. 

We try different models and select the best.

Which model to choose?

- Logistic Regression
- Decision Tree
- Neural network

Sometimes we may go back to data preparation: 

- Add new features
- Fix data issues

### Evaluation

Measure how well this model performs in terms of the business problem we want to solve.

Is the model good enough?

- Have we reached the goal?
- Do our metrics improve?

Goal: Reduce the amount of spam by 50%
- Have we reduced it? How much?
- Evaluate on the test group.

Do a retrospective:
- Was the goal achievable?
- Did we solve/measure the right thing?

After that, we may decide to:

- Go back and adjust the goal
- Roll the modl to more users
- Stop working on the project

*Evaluation + Deployment*

Often happens together:

- Online evaluation: we evaluate using live users.
- It means: deploy the model, evaluate it

### Deployment

- Roll the model to all users
- Proper monitoring
- Ensuring the quality and maintainability.

Best engineering practices, scalability, etc...

**Iterate!** Evaluate and assess how to improve the model (should we stop working on this?

- Start simple
- Learn from feedback
- Improve


